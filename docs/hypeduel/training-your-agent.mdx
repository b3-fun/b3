---
title: "Training your Agent"
description: "Learn how to setup and train Unity ML agents to compete in HypeDuel arenas using deep reinforcement learning"
---

<iframe class="w-full aspect-video rounded-xl mb-6" src="https://customer-gg6qs7nm5ue94t64.cloudflarestream.com/fa641966225b3fe0c28f7c1eeaac6216/iframe?muted=true&loop=true&autoplay=true&poster=https%3A%2F%2Fcustomer-gg6qs7nm5ue94t64.cloudflarestream.com%2Ffa641966225b3fe0c28f7c1eeaac6216%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D%26height%3D600" title="Training Setup" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen />
[Contact us before you begin](https://b3builders.typeform.com/hypeduel)

## Getting Started

### Prerequisites

Before you begin training your agent, ensure you have:

- **Unity 2022.3 LTS** or newer
- **Python 3.8+** with pip installed
- **ML-Agents Toolkit** from Unity
- **PyTorch** or **TensorFlow** for training

### Installation Steps

<Steps>
  <Step title="Install Unity ML-Agents">
    ```bash
    pip install mlagents
    ```
  </Step>
  <Step title="Clone HypeDuel Training Template">
    ```bash
    git clone https://github.com/hypeduel/agent-training-template
    cd agent-training-template
    ```
  </Step>
  <Step title="Install Dependencies">
    ```bash
    pip install -r requirements.txt
    ```
  </Step>
  <Step title="Open in Unity">
    Open the project in Unity Hub and load the training scene
  </Step>
</Steps>

## Training Configuration

### Agent Behavior Parameters

Configure your agent's learning parameters in the `config.yaml` file:

```yaml
behaviors:
  HypeDuelAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 3.0e-4
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
```

### Observation Space

Your agent will receive observations including:

- **Arena State**: Current positions, health, and power-ups
- **Opponent Behavior**: Movement patterns and recent actions
- **Environmental Data**: Arena boundaries, obstacles, and hazards
- **Game State**: Time remaining, score differential, and power-up availability

### Action Space

Agents can perform actions such as:

- **Movement**: Forward, backward, left, right
- **Combat**: Attack, defend, special abilities
- **Strategy**: Positioning, power-up collection, defensive maneuvers

## Training Process

### Local Training

Start training your agent locally:

```bash
mlagents-learn config.yaml --run-id=hypeduel-agent-v1
```

### Training Metrics

Monitor your agent's progress with key metrics:

<CardGroup cols={2}>
  <Card title="Cumulative Reward" icon="chart-line">
    Track total rewards earned per episode
  </Card>
  <Card title="Episode Length" icon="clock">
    Monitor how long your agent survives in battles
  </Card>
  <Card title="Win Rate" icon="trophy">
    Percentage of battles won against baseline opponents
  </Card>
  <Card title="Policy Loss" icon="trending-down">
    Measure learning convergence and stability
  </Card>
</CardGroup>

### Arena Submission

Once your agent is trained:

1. **Test Performance**: Validate against benchmark opponents
2. **Model Optimization**: Ensure efficient inference speed
3. **Submit to HypeDuel**: Upload your `.onnx` model file
4. **Arena Registration**: Register your agent for specific arenas

## Best Practices

### Training Tips

- **Start Simple**: Begin with basic behaviors before complex strategies
- **Reward Engineering**: Design rewards that encourage desired behaviors
- **Regular Evaluation**: Test against diverse opponents frequently
- **Incremental Complexity**: Gradually introduce advanced mechanics

### Performance Optimization

- **Observation Normalization**: Scale inputs for stable training
- **Action Space Design**: Balance expressiveness with training efficiency
- **Memory Management**: Optimize for both training and inference
- **Model Compression**: Reduce model size for faster arena deployment

## Troubleshooting

### Common Issues

<Accordion title="Training Not Converging">
  Check learning rate, reward scaling, and observation normalization. Consider reducing complexity or adjusting hyperparameters.
</Accordion>

<Accordion title="Agent Performance Plateau">
  Implement curriculum learning, increase environment diversity, or try self-play training against stronger opponents.
</Accordion>

<Accordion title="Inference Too Slow">
  Optimize model architecture, use ONNX runtime, or implement action caching for repeated decisions.
</Accordion>
